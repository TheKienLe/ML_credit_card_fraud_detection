{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheKienLe/ML_credit_card_fraud_detection/blob/main/notebooks/Chapter%2010%20-%20FineTuning_a_LLM_LIMA_CPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> export WANDB_PROJECT=GenAI360\n",
        "---\n",
        "> import torch; torch.set_num_threads(8);\n",
        "\n"
      ],
      "metadata": {
        "id": "xlptpnRFf7qM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0Y36dlkZ4vy"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.38.2 accelerate==0.28.0 transformers-stream-generator==0.0.4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Deep Lake Dataset"
      ],
      "metadata": {
        "id": "KJqhCFrH5Ckf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import deeplake\n",
        "\n",
        "# Connect to the training and testing datasets\n",
        "ds = deeplake.load('hub://genai360/GAIR-lima-train-set')\n",
        "ds_test = deeplake.load('hub://genai360/GAIR-lima-test-set')"
      ],
      "metadata": {
        "id": "d5JZL1URSTAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "id": "u5IozxXrQkYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_sample_text(example):\n",
        "    \"\"\"Prepare the text from a sample of the dataset.\"\"\"\n",
        "    text = f\"Question: {example['question'].text()}\\n\\nAnswer: {example['answer'].text()}\"\n",
        "    return text"
      ],
      "metadata": {
        "id": "tPxz1XywNuqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")"
      ],
      "metadata": {
        "id": "qxZCnDLrOvyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl.trainer import ConstantLengthDataset\n",
        "\n",
        "train_dataset = ConstantLengthDataset(\n",
        "    tokenizer,\n",
        "    ds,\n",
        "    formatting_func=prepare_sample_text,\n",
        "    infinite=True,\n",
        "    seq_length=1024\n",
        ")"
      ],
      "metadata": {
        "id": "fpmkEcW2Fckq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterator = iter( train_dataset )\n",
        "sample = next( iterator )\n",
        "print( sample )"
      ],
      "metadata": {
        "id": "fhm4vcZQRRX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.stbart_iteration = 0"
      ],
      "metadata": {
        "id": "PsrLFquURRUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = ConstantLengthDataset(\n",
        "    tokenizer,\n",
        "    ds_test,\n",
        "    formatting_func=prepare_sample_text,\n",
        "    seq_length=1024\n",
        ")"
      ],
      "metadata": {
        "id": "pqwPCTmKUFno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add LoRA Layers"
      ],
      "metadata": {
        "id": "55H1tOm03MoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "QH2tB7jx3Im_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "id": "mTj18xs9SnkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./OPT-fine_tuned-LIMA-CPU\",\n",
        "    dataloader_drop_last=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    num_train_epochs=10,\n",
        "    logging_steps=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    learning_rate=1e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_steps=100,\n",
        "    gradient_accumulation_steps=1,\n",
        "    bf16=True,\n",
        "    weight_decay=0.05,\n",
        "    run_name=\"OPT-fine_tuned-LIMA-CPU\",\n",
        "    report_to=\"wandb\",\n",
        ")"
      ],
      "metadata": {
        "id": "Z2gl7c1ZS7jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", torch_dtype=torch.bfloat16)"
      ],
      "metadata": {
        "id": "4zCFalHDTwwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ],
      "metadata": {
        "id": "4TdQ_iupEBbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=lora_config,\n",
        "    packing=True,\n",
        ")"
      ],
      "metadata": {
        "id": "sGY4btuwUVNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_trainable_parameters(trainer.model)"
      ],
      "metadata": {
        "id": "IlsjSyIlYF2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "gb4jVAxnYGdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# print(\"Saving last checkpoint of the model\")\n",
        "# trainer.model.save_pretrained(os.path.join(\"./OPT-fine_tuned-LIMA\", \"final_checkpoint/\"))"
      ],
      "metadata": {
        "id": "KulKuyOhYKD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge LoRA weights with Base Model"
      ],
      "metadata": {
        "id": "oApD5JAjh1pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoModelForCausalLM\n",
        "# import torch\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained( \"facebook/opt-1.3b\", return_dict=True)"
      ],
      "metadata": {
        "id": "pcm9Dy1niKDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from peft import PeftModel\n",
        "\n",
        "# # Load the Lora model\n",
        "# model = PeftModel.from_pretrained(model, \"./OPT-fine_tuned-LIMA/final_checkpoint/\")\n",
        "# model.eval()"
      ],
      "metadata": {
        "id": "MRqn_wDjiM8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = model.merge_and_unload()\n",
        "\n",
        "# model.save_pretrained(\"./OPT-fine_tuned-LIMA/merged\")"
      ],
      "metadata": {
        "id": "OGWgnEvsRptO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
        "# tokenizer.save_pretrained(\"./OPT-fine_tuned/merged\")"
      ],
      "metadata": {
        "id": "qiYuTY6CRpbs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}